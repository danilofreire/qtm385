library(randomizr)
library(readr)
library(estimatr)
# Load necessary packages
library(ri2)
library(randomizr)
# Define treatment and control groups
treatment <- c(2, 11, 14, 0, 3) - 7  # Subtract 7 from each outcome
control <- c(1, 0, 0, 4, 3)
# Create dataset
diet.data <- data.frame(
weight_loss = c(treatment, control),
treat = c(rep(1, length(treatment)), rep(0, length(control)))
)
# Declare the randomization procedure
declaration <- declare_ra(N = nrow(diet.data), m = sum(diet.data$treat))
# Conduct randomization inference (following lecture format)
ri_result <- conduct_ri(
formula = weight_loss ~ treat,
assignment = "treat",
declaration = declaration,
sharp_hypothesis = 0,
data = diet.data
)
# Summary of results
summary(ri_result)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ri2)
library(randomizr)
library(readr)
library(estimatr)
# Load necessary packages
library(ri2)
library(randomizr)
# Define treatment and control groups
treatment <- c(2, 11, 14, 0, 3) - 7  # Subtract 7 from each outcome
control <- c(1, 0, 0, 4, 3)
# Create dataset
diet.data <- data.frame(
weight_loss = c(treatment, control),
treat = c(rep(1, length(treatment)), rep(0, length(control)))
)
# Declare the randomization procedure
declaration <- declare_ra(N = nrow(diet.data), m = sum(diet.data$treat))
# Conduct randomization inference (following lecture format)
ri_result <- conduct_ri(
formula = weight_loss ~ treat,
assignment = "treat",
declaration = declaration,
sharp_hypothesis = 0,
data = diet.data
)
# we use the specificed values
mu_t <- 55
mu_c <- 50
sigma <- 10
alpha <- 0.05
N <- 100
power_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N) {
lowertail <- (abs(mu_t - mu_c) * sqrt(N)) / (2 * sigma)
uppertail <- -1 * lowertail
beta <- pnorm(lowertail - qnorm(1 - alpha/2), lower.tail=TRUE) +
1 - pnorm(uppertail - qnorm(1 - alpha/2), lower.tail=FALSE)
return(beta)
}
power_value <- power_calculator(mu_t, mu_c, sigma, alpha, N)
print(power_value)
# do the same thing as above
mu_t <- 37 * 1.05  # 0.5 because 5% increase in test scores
mu_c <- 37
sigma <- 16.5
N <- 2400 #this is our constraint on how much we can afford
power_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N) {
lowertail <- (abs(mu_t - mu_c) * sqrt(N)) / (2 * sigma)
uppertail <- -1 * lowertail
beta <- pnorm(lowertail - qnorm(1 - alpha/2), lower.tail=TRUE) +
1 - pnorm(uppertail - qnorm(1 - alpha/2), lower.tail=FALSE)
return(beta)
}
power_value <- power_calculator(mu_t, mu_c, sigma, alpha, N)
print(power_value)
library(readr)
library(estimatr)
# Load dataset
titiunik <- read_csv("C:/Users/Abasu/Documents/GitHub/qtm385/assignments/04-assignment/titiunik.csv")
# Estimate effect in Texas
texas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 0))
summary(texas_model)
# Estimate effect in Arkansas
arkansas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 1))
summary(arkansas_model)
# Combined model (pooling both states)
combined_model <- lm_robust(bills_introduced ~ term2year + factor(texas0_arkansas1), data = titiunik)
summary(combined_model)
library(ri2)
library(randomizr)
# Declare randomization procedure
declaration <- declare_ra(N = nrow(titiunik), m = sum(titiunik$term2year))
# Conduct randomization inference
ri_result <- conduct_ri(
formula = bills_introduced ~ term2year,
assignment = "term2year",
declaration = declaration,  # Fixed error: Added required random assignment declaration
sharp_hypothesis = 0,
data = titiunik,
sims = 1000  # 1000 simulations for inference
)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ri2)
library(randomizr)
library(readr)
library(estimatr)
# Load necessary packages
library(ri2)
library(randomizr)
# Define treatment and control groups
treatment <- c(2, 11, 14, 0, 3) - 7  # Subtract 7 from each outcome
control <- c(1, 0, 0, 4, 3)
# Create dataset
diet.data <- data.frame(
weight_loss = c(treatment, control),
treat = c(rep(1, length(treatment)), rep(0, length(control)))
)
# Declare the randomization procedure
declaration <- declare_ra(N = nrow(diet.data), m = sum(diet.data$treat))
# Conduct randomization inference (following lecture format)
ri_result <- conduct_ri(
formula = weight_loss ~ treat,
assignment = "treat",
declaration = declaration,
sharp_hypothesis = 0,
data = diet.data
)
# we use the specificed values
mu_t <- 55
mu_c <- 50
sigma <- 10
alpha <- 0.05
N <- 100
power_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N) {
lowertail <- (abs(mu_t - mu_c) * sqrt(N)) / (2 * sigma)
uppertail <- -1 * lowertail
beta <- pnorm(lowertail - qnorm(1 - alpha/2), lower.tail=TRUE) +
1 - pnorm(uppertail - qnorm(1 - alpha/2), lower.tail=FALSE)
return(beta)
}
power_value <- power_calculator(mu_t, mu_c, sigma, alpha, N)
print(power_value)
# do the same thing as above
mu_t <- 37 * 1.05  # 0.5 because 5% increase in test scores
mu_c <- 37
sigma <- 16.5
N <- 2400 #this is our constraint on how much we can afford
power_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N) {
lowertail <- (abs(mu_t - mu_c) * sqrt(N)) / (2 * sigma)
uppertail <- -1 * lowertail
beta <- pnorm(lowertail - qnorm(1 - alpha/2), lower.tail=TRUE) +
1 - pnorm(uppertail - qnorm(1 - alpha/2), lower.tail=FALSE)
return(beta)
}
power_value <- power_calculator(mu_t, mu_c, sigma, alpha, N)
print(power_value)
library(readr)
library(estimatr)
# Load dataset
titiunik <- read_csv("C:/Users/Abasu/Documents/GitHub/qtm385/assignments/04-assignment/titiunik.csv")
# Estimate effect in Texas
texas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 0))
summary(texas_model)
# Estimate effect in Arkansas
arkansas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 1))
summary(arkansas_model)
# Combined model (pooling both states)
combined_model <- lm_robust(bills_introduced ~ term2year + factor(texas0_arkansas1), data = titiunik)
summary(combined_model)
library(ri2)
library(randomizr)
# Declare randomization procedure
declaration <- declare_ra(N = nrow(titiunik), m = sum(titiunik$term2year))
# Conduct randomization inference
ri_result <- conduct_ri(
formula = bills_introduced ~ term2year,
assignment = "term2year",
declaration = declaration,  # Fixed error: Added required random assignment declaration
sharp_hypothesis = 0,
data = titiunik,
sims = 1000  # 1000 simulations for inference
)
knitr::opts_chunk$set(echo = TRUE)
# Load required packages
library(dplyr)
library(readr)
library(estimatr)
library(ri2)
library(randomizr)
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(estimatr)
library(ri2)
library(randomizr)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(estimatr)
library(ri2)
library(randomizr)
# Treatment and control groups
treatment <- c(2, 11, 14, 0, 3) - 7
control <- c(1, 0, 0, 4, 3)
# Create dataset
diet.data <- data.frame(
weight_loss = c(treatment, control),
treat = c(rep(1, length(treatment)), rep(0, length(control)))
)
# Declare randomization
declaration <- declare_ra(N = nrow(diet.data), m = sum(diet.data$treat))
# Conduct RI
ri_result <- conduct_ri(
formula = weight_loss ~ treat,
assignment = "treat",
declaration = declaration,
sharp_hypothesis = 0,
data = diet.data
)
summary(ri_result)
titiunik <- read_csv("C:/Users/Abasu/Documents/GitHub/qtm385/assignments/04-assignment/titiunik.csv")
# Texas
texas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 0))
summary(texas_model)
# Arkansas
arkansas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 1))
summary(arkansas_model)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(estimatr)
library(ri2)
library(randomizr)
# Treatment and control groups
treatment <- c(2, 11, 14, 0, 3) - 7
control <- c(1, 0, 0, 4, 3)
# Create dataset
diet.data <- data.frame(
weight_loss = c(treatment, control),
treat = c(rep(1, length(treatment)), rep(0, length(control)))
)
# Declare randomization
declaration <- declare_ra(N = nrow(diet.data), m = sum(diet.data$treat))
# Conduct RI
ri_result <- conduct_ri(
formula = weight_loss ~ treat,
assignment = "treat",
declaration = declaration,
sharp_hypothesis = 0,
data = diet.data
)
summary(ri_result)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(estimatr)
library(ri2)
library(randomizr)
# Treatment and control groups
treatment <- c(2, 11, 14, 0, 3) - 7
control <- c(1, 0, 0, 4, 3)
# Create dataset
diet.data <- data.frame(
weight_loss = c(treatment, control),
treat = c(rep(1, length(treatment)), rep(0, length(control)))
)
# Declare randomization
declaration <- declare_ra(N = nrow(diet.data), m = sum(diet.data$treat))
# Conduct RI
ri_result <- conduct_ri(
formula = weight_loss ~ treat,
assignment = "treat",
declaration = declaration,
sharp_hypothesis = 0,
data = diet.data
)
# we use the specificed values
mu_t <- 55
mu_c <- 50
sigma <- 10
alpha <- 0.05
N <- 100
power_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N) {
lowertail <- (abs(mu_t - mu_c) * sqrt(N)) / (2 * sigma)
uppertail <- -1 * lowertail
beta <- pnorm(lowertail - qnorm(1 - alpha/2), lower.tail=TRUE) +
1 - pnorm(uppertail - qnorm(1 - alpha/2), lower.tail=FALSE)
return(beta)
}
power_value <- power_calculator(mu_t, mu_c, sigma, alpha, N)
print(power_value)
# do the same thing as above
mu_t <- 37 * 1.05  # 0.5 because 5% increase in test scores
mu_c <- 37
sigma <- 16.5
N <- 2400 #this is our constraint on how much we can afford
power_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N) {
lowertail <- (abs(mu_t - mu_c) * sqrt(N)) / (2 * sigma)
uppertail <- -1 * lowertail
beta <- pnorm(lowertail - qnorm(1 - alpha/2), lower.tail=TRUE) +
1 - pnorm(uppertail - qnorm(1 - alpha/2), lower.tail=FALSE)
return(beta)
}
power_value <- power_calculator(mu_t, mu_c, sigma, alpha, N)
print(power_value)
# the power is usually worth it if its around 0.8
titiunik <- read_csv("C:/Users/Abasu/Documents/GitHub/qtm385/assignments/04-assignment/titiunik.csv")
# Texas
texas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 0))
summary(texas_model)
# Arkansas
arkansas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 1))
summary(arkansas_model)
# pooling both states)
combined_model <- lm_robust(bills_introduced ~ term2year + factor(texas0_arkansas1), data = titiunik)
summary(combined_model)
library(ri2)
library(randomizr)
# Declare randomization procedure
declaration <- declare_ra(N = nrow(titiunik), m = sum(titiunik$term2year))
# Conduct randomization inference
ri_result <- conduct_ri(
formula = bills_introduced ~ term2year,
assignment = "term2year",
declaration = declaration,  # Fixed error: Added required random assignment declaration
sharp_hypothesis = 0,
data = titiunik,
sims = 1000  # 1000 simulations for inference
)
# Treatment and control groups
treatment <- c(2, 11, 14, 0, 3) - 7
control <- c(1, 0, 0, 4, 3)
# Create dataset
diet.data <- data.frame(
weight_loss = as.numeric(c(treatment, control)),  # Ensure numeric
treat = as.numeric(c(rep(1, length(treatment)), rep(0, length(control))))  # Ensure numeric
)
# Declare randomization procedure
declaration <- declare_ra(N = nrow(diet.data), m = sum(diet.data$treat))
ri_result <- conduct_ri(
formula = weight_loss ~ treat,
assignment = "treat",
declaration = declaration,
sharp_hypothesis = 0,
data = diet.data,
sims = 1000  # Reduced to 1000 for efficiency
)
summary(ri_result)
# Treatment and control groups
treatment <- c(2, 11, 14, 0, 3) - 7
control <- c(1, 0, 0, 4, 3)
# Create dataset
diet.data <- data.frame(
weight_loss = as.numeric(c(treatment, control)),  # Ensure numeric
treat = as.numeric(c(rep(1, length(treatment)), rep(0, length(control))))  # Ensure numeric
)
# Declare randomization procedure
declaration <- declare_ra(N = nrow(diet.data), m = sum(diet.data$treat))
# Conduct RI
ri_result <- conduct_ri(
formula = weight_loss ~ treat,
assignment = "treat",
declaration = declaration,
sharp_hypothesis = 0,
data = diet.data,
sims = 1000  # Reduced to 1000 for efficiency
)
summary(ri_result)
# Treatment and control groups
treatment <- c(2, 11, 14, 0, 3) - 7
control <- c(1, 0, 0, 4, 3)
# Create dataset
diet.data <- data.frame(
weight_loss = as.numeric(c(treatment, control)),  # Ensure numeric
treat = as.numeric(c(rep(1, length(treatment)), rep(0, length(control))))  # Ensure numeric
)
# Declare randomization procedure
declaration <- declare_ra(N = nrow(diet.data), m = sum(diet.data$treat))
# Conduct RI
ri_result <- conduct_ri(
formula = weight_loss ~ treat,
assignment = "treat",
declaration = declaration,
sharp_hypothesis = 0,
data = diet.data,
sims = 1000  # Reduced to 1000 for efficiency
)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(estimatr)
library(ri2)
library(randomizr)
# Treatment and control groups
treatment <- c(2, 11, 14, 0, 3) - 7
control <- c(1, 0, 0, 4, 3)
# Create dataset
diet.data <- data.frame(
weight_loss = as.numeric(c(treatment, control)),  # Ensure numeric
treat = as.numeric(c(rep(1, length(treatment)), rep(0, length(control))))  # Ensure numeric
)
# Declare randomization procedure
declaration <- declare_ra(N = nrow(diet.data), m = sum(diet.data$treat))
ri_result <- conduct_ri(
formula = weight_loss ~ treat,
assignment = "treat",
declaration = declaration,
sharp_hypothesis = 0,
data = diet.data,
sims = 1000
)
# we use the specificed values
mu_t <- 55
mu_c <- 50
sigma <- 10
alpha <- 0.05
N <- 100
power_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N) {
lowertail <- (abs(mu_t - mu_c) * sqrt(N)) / (2 * sigma)
uppertail <- -1 * lowertail
beta <- pnorm(lowertail - qnorm(1 - alpha/2), lower.tail=TRUE) +
1 - pnorm(uppertail - qnorm(1 - alpha/2), lower.tail=FALSE)
return(beta)
}
power_value <- power_calculator(mu_t, mu_c, sigma, alpha, N)
print(power_value)
# do the same thing as above
mu_t <- 37 * 1.05  # 0.5 because 5% increase in test scores
mu_c <- 37
sigma <- 16.5
N <- 2400 #this is our constraint on how much we can afford
power_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N) {
lowertail <- (abs(mu_t - mu_c) * sqrt(N)) / (2 * sigma)
uppertail <- -1 * lowertail
beta <- pnorm(lowertail - qnorm(1 - alpha/2), lower.tail=TRUE) +
1 - pnorm(uppertail - qnorm(1 - alpha/2), lower.tail=FALSE)
return(beta)
}
power_value <- power_calculator(mu_t, mu_c, sigma, alpha, N)
print(power_value)
# the power is usually worth it if its around 0.8
titiunik <- read_csv("C:/Users/Abasu/Documents/GitHub/qtm385/assignments/04-assignment/titiunik.csv")
# Texas
texas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 0))
summary(texas_model)
# Arkansas
arkansas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 1))
summary(arkansas_model)
# pooling both states
combined_model <- lm_robust(bills_introduced ~ term2year + factor(texas0_arkansas1), data = titiunik)
summary(combined_model)
library(ri2)
library(randomizr)
declaration <- declare_ra(N = nrow(titiunik), m = sum(titiunik$term2year))
ri_result <- conduct_ri(
formula = bills_introduced ~ term2year,
assignment = "term2year",
declaration = declaration,
sharp_hypothesis = 0,
data = titiunik,
sims = 1000
)
# we use the specificed values
mu_t <- 55
mu_c <- 50
sigma <- 10
alpha <- 0.05
N <- 100
power_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N) {
lowertail <- (abs(mu_t - mu_c) * sqrt(N)) / (2 * sigma)
uppertail <- -1 * lowertail
beta <- pnorm(lowertail - qnorm(1 - alpha/2), lower.tail=TRUE) +
1 - pnorm(uppertail - qnorm(1 - alpha/2), lower.tail=FALSE)
return(beta)
}
power_value <- power_calculator(mu_t, mu_c, sigma, alpha, N)
print(power_value)
# do the same thing as above
mu_t <- 37 * 1.05  # 0.5 because 5% increase in test scores
mu_c <- 37
sigma <- 16.5
N <- 2400 #this is our constraint on how much we can afford
power_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N) {
lowertail <- (abs(mu_t - mu_c) * sqrt(N)) / (2 * sigma)
uppertail <- -1 * lowertail
beta <- pnorm(lowertail - qnorm(1 - alpha/2), lower.tail=TRUE) +
1 - pnorm(uppertail - qnorm(1 - alpha/2), lower.tail=FALSE)
return(beta)
}
power_value <- power_calculator(mu_t, mu_c, sigma, alpha, N)
print(power_value)
# the power is usually worth it if its around 0.8
titiunik <- read_csv("C:/Users/Abasu/Documents/GitHub/qtm385/assignments/04-assignment/titiunik.csv")
# Texas
texas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 0))
summary(texas_model)
# Arkansas
arkansas_model <- lm_robust(bills_introduced ~ term2year, data = titiunik %>% filter(texas0_arkansas1 == 1))
summary(arkansas_model)
# pooling both states
combined_model <- lm_robust(bills_introduced ~ term2year + factor(texas0_arkansas1), data = titiunik)
summary(combined_model)
