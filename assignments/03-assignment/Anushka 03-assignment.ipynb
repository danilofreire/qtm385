{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"QTM 385 - Experimental Methods\"\n",
        "subtitle: Assignment 03\n",
        "---\n",
        "\n",
        "# Instructions\n",
        "\n",
        "This assignment covers the last two lectures of the course. As usual, it consists of 10 questions, each worth one point. You can answer the questions in any format you prefer, but I recommend using Jupyter Notebooks and converting the answers to PDF or html, as they are easier to read on Canvas. Please write at least one or two paragraphs for each written question.\n",
        "\n",
        "If you have any questions about the assignment, feel free to email me at <danilo.freire@emory.edu>.\n",
        "\n",
        "Good luck!\n",
        "\n",
        "# Questions \n",
        "\n",
        "# 1. Compare and contrast Type I and Type II errors. In causal inference experiments, why might a researcher be more concerned with one type of error over the other?\n",
        "\n",
        "A type 1 error is when we falsely reject the null hypothesis when it is in fact true, while a type 2 error is when we fail to reject a null hypothesis that is false. \n",
        "\n",
        "A false positive is generally the more serious scenario, especially for researchers looking for evidence that a treatment is effective (like a new medicine or policy), especially if it is something that can effect everyone. According to wikipedia, \"A vaccine is generally considered effective if the estimate is ≥50% with a >30% lower limit of the 95% confidence interval.\" When COVID vaccines came out, people all over the world took them (and boosters) with the assumption that it was safe to be unmasked and stop social distancing. However, if one of them was not actually in that confidence interval - millions of people would be screwed!\n",
        "\n",
        "However, in a medical situation for example, a false negative can be dangerous - saying someone doesn't have a life-threatening disease like cancer when they actually do, and not catching the error until it is too late to save them, would be a big concern. Additionally, like the pregnacy picture in the slides, telling a woman she's not pregnant when she actually is could be extremely problematic - especially if she didn't want the baby and well... lived in America. \n",
        "\n",
        "However, type errors aren't always bad. A fun fact I learned in my AP stats is that meteorologists often overestimate when prediciting the forecast, especially with the chance of rain. Suppose the null hypothesis is that it won't rain on Tuesday, and assuming there's a cutoff percentage that determines whether or not it will rain, the weather app might tell us it will probably rain (ex: 60% ) when in reality it probably won't (30%). That's technically a type error, since we are being falsely told there's a significantly signifant chance it will rain - but this is done all the time, because most people would be pleasantly surprised for it to not rain when they are prepared for it, but upset if it rained when the app said it wouldn't!\n",
        "\n",
        "\n",
        "# 2. Explain the concept of randomisation inference and outline its advantages over traditional parametric tests, especially in the context of testing the sharp null hypothesis.\n",
        "\n",
        "Randomisation inference is when we calculate  p values based on an inventory of possible randomizations, or every possibility that could happen, and is applicable to any sort of size, whether minimal sampling or an entire country. You don't have to rely on the assumption of nomality and isn't parametric. When testing the sharp null hypothesis, randomization inference provides a more precise way to determine statistical significance.\n",
        "\n",
        "\n",
        "# 3. Compare Neyman’s hypothesis testing framework with Fisher’s sharp null hypothesis approach. What are the main advantages and disadvantages of each method in experimental settings?\n",
        "\n",
        "Neyman's appraoch focused more on finding average treatment effect and constructing confidence intervals. However, it relies on possibilities rather than the certainity of something happening, meaning that you have to work with a range of outcomes from a treatment (like discussed in class, trillions of randomizations can happen)\n",
        "\n",
        "Fisher's approach in comparison assumes for the treatment to have zero effect on everyone, making it restricted to essentially all or nothing in experiments, which makes it less helpful in smaller experiments.\n",
        "\n",
        "# 4. Critically evaluate the use of p-values in hypothesis testing. What alternatives are suggested (or implied) in the lectures, and what are the potential benefits of these alternatives?\n",
        "\n",
        "P values are used all the time but they aren't always used accurately and understood. I feel like many people think that the commonly used cutoff of 0.05 means that there is a 95% chance the null isn't true - but it actually means that if the null is true, we would see an effect 5% of the time. Additionally, people might skew the p values to make it bigger or smaller to get a number that is considered significant - the notorious p hacking! Rndomization inference and confidence intervals are viable alternatives - confidence intervals give a range of values that aren't just \"significant\" or \"not significant\" while randomization inference uses outcomes that we can't directly observe a lot of the time and gives us better p values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. The code below simulates a dataset. Modify the code so that it adds a new variable called `treat` with 500 treated individuals and 500 control individuals (complete random assignment). Also include a binary covariate called `gender` (0 = male, 1 = female; with equal probability) and update the outcome (`interviews`) by adding 2 points if the individual is female.\n",
        "\n",
        "\n",
        "\n",
        "```r\n",
        "## Set seed for reproducibility\n",
        "set.seed(385)\n",
        "\n",
        "# Load packages\n",
        "# install.packages(\"fabricatr\")\n",
        "# install.packages(\"randomizr\") # if you haven't installed them yet\n",
        "library(fabricatr)\n",
        "library(randomizr)\n",
        "\n",
        "## Simulate data\n",
        "data <- fabricate(\n",
        "  N = 1000,\n",
        "  interviews = round(rnorm(1000, mean = 10, sd = 2) + 5 * treat, digits = 0)\n",
        ")\n",
        "head(data)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2305238311.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    ```r\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# my version\n",
        "```r\n",
        "\n",
        "set.seed(385)\n",
        "library(fabricatr)\n",
        "library(randomizr)\n",
        "\n",
        "data <- fabricate(\n",
        "  N = 1000,\n",
        "  treat = complete_ra(N = 1000, m = 500),  # Randomly assign 500 \n",
        "  gender = rbinom(1000, 1, 0.5),           # 0 = male, 1 = female\n",
        "  interviews = round(rnorm(1000, mean = 10, sd = 2) + 5 * treat + 2 * gender, digits = 0)\n",
        ")\n",
        "head(data)\n",
        "```\n",
        "\n",
        "# This adds random treatment assignment and a binary gender variable, increasing interviews by 2 for females."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Using the dataset created in the previous question, estimate the average treatment effect on the outcome `interviews` using the `lm_robust()` function from the `estimatr` package. Interpret the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(estimatr)\n",
        "\n",
        "model <- lm_robust(interviews ~ treat, data = data)\n",
        "\n",
        "summary(model)\n",
        "\n",
        "# The coefficient on treat represents the average treatment effect. A significant positive coefficient suggests the treatment increases the number of interviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Using the same dataset, estimate the average treatment effect of the treatment on the outcome interviews using randomisation inference. Interpret the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(ri2)\n",
        "\n",
        "ri_result <- conduct_ri(\n",
        "  formula = interviews ~ treat,\n",
        "  assignment = \"treat\",\n",
        "  sharp_hypothesis = 0,\n",
        "  data = data\n",
        ")\n",
        "\n",
        "summary(ri_result)\n",
        "\n",
        "# randomization inference tests if our observed effect is due to chance. A small p-value means the treatment was probably significant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Explain how including covariates in an experimental regression model can increase the precision of the treatment effect estimate. Under what conditions might this adjustment lead to biased estimates?\n",
        "\n",
        "Including covariates in an experimental regression model can help precision of the treatment effect estimate by accounting for variability in the outcome that isn't because of treatment By controlling for these additional variables, we reduce the residual variance, leading to more narrow confidence intervals for the treatment effect. However, this can lead to biased estimates if the covariates are post-treatment variables or if they are influenced by both the treatment and the outcome. Therefore, it's crucial to include only pre-treatment covariates that are not influenced by the treatment, like subgroups such as race."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Simulate a dataset with heterogeneous treatment effects (e.g., the treatment effect is larger for individuals with higher education). Estimate the treatment effect for different subgroups using an interaction term.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data <- fabricate(\n",
        "  N = 1000,\n",
        "  treat = complete_ra(N, m = 500),\n",
        "  education = rnorm(1000, mean = 12, sd = 2),  # Years of education\n",
        "  interviews = round(rnorm(1000, mean = 10, sd = 2) + 5 * treat + 0.5 * education + 2 * treat * education, 0)\n",
        ")\n",
        "\n",
        "model_het <- lm_robust(interviews ~ treat * education, data = data)\n",
        "summary(model_het)\n",
        "\n",
        "# The interaction term (treat:education) shows whether higher education increases the treatment effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mThe kernel 'R' was not started as it is located in an insecure location 'c:\\ProgramData\\jupyter\\kernels\\ir\\kernel.json'.  \n",
            "\u001b[1;31mClick <a href='https://aka.ms/JupyterTrustedKernelPaths'>here</a> for further details, optionally update the setting <a href='command:workbench.action.openSettings?[\"jupyter.kernels.trusted\"]'>jupyter.kernels.trusted</a> to trust the kernel."
          ]
        }
      ],
      "source": [
        "\n",
        "data <- fabricate(\n",
        "  N = 1000,\n",
        "  treat = complete_ra(N, m = 500),\n",
        "  education = rnorm(1000, mean = 12, sd = 2),  # Years of education\n",
        "  interviews = round(rnorm(1000, mean = 10, sd = 2) + 5 * treat + 0.5 * education + 2 * treat * education, 0)\n",
        ")\n",
        "\n",
        "model_het <- lm_robust(interviews ~ treat * education, data = data)\n",
        "summary(model_het)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Why is the publication of null results important in experimental research? What are the main challenges in publishing null results, and how can the scientific community address these challenges?\n",
        "\n",
        "Publishing null results is so important because it prevents publication bias, where only experiments with \"meaningful\" results are published. This bias causes a lot of false positives and experiments that cannot be replicated. The challenge is that journals would rather publish experiments that went somewhere rather than \"failed\" ones, making it harder to publish studies that find no effect. Solutions to combat this include pre-registration, where you have to publish details beforehand, and journals that highlight null results, which can also help us put more trust into the things we see. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
